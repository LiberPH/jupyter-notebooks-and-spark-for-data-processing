{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jupyter Notebooks and Spark for Data Processing\n",
    "![Spark](images/spark.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Objectives of this Talk are\n",
    "\n",
    "* Provide a broad explanation of:\n",
    "    * What is a Jupyter Notebook and what is it good for.\n",
    "    * What is Spark and what is it good for.\n",
    "* Provide an example on how to process data and run a model with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before We Start\n",
    "\n",
    "1. Download Docker. https://www.docker.com/products\n",
    "2. Download the pyspark container by typing `docker pull jupyter/all-spark-notebook` in the terminal\n",
    "3. Run the container in the terminal: `docker run -it --rm -p 8888:8888 jupyter/all-spark-notebook`\n",
    "4. Copy the URL displayed on terminal\n",
    "5. Paste the URL in your navigator\n",
    "6. Click on New\n",
    "7. Select [Python](images/create_python_notebook.png).\n",
    "8. Make sure you comply with at least steps 1-3 before we start this crash course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  A Jupyter Notebook is a Web Application\n",
    "![Notebook](images/notebooks.jpeg)\n",
    "\n",
    " > A [Jupyter Notebook](https://jupyter.org/) is a web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Jupyter Notebook:\n",
    "\n",
    "* Provides a sandbox-like environment for experimenting\n",
    "* Has markdown enabled that facilitates code or process documentation\n",
    "* Allows to add images\n",
    "* Allows to run code\n",
    "* !!! Can be run in disorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark is a Unified Analytics Engine for Large-scale Data Processing\n",
    "![Spark](images/Spark2.jpeg)\n",
    "\n",
    "\n",
    "https://data-flair.training/blogs/apache-spark-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark is a Unified Analytics Engine for Large-scale Data Processing\n",
    "\n",
    "Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching and optimized query execution for fast queries against data of any size. Simply put, Spark is a fast and general engine for large-scale data processing.\n",
    "\n",
    "It is a tool that can be excecuted with Python, R, Scala or Java and allows us to work with Big Data.\n",
    "\n",
    "https://chartio.com/learn/data-analytics/what-is-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data is a Ludicrous Amount of Data\n",
    "For the sake of this talk [Big Data](https://arxiv.org/pdf/1309.5821.pdf&gt) is any amount of data my computer or dedicated server can't read or process on its own.\n",
    "\n",
    "(A Ludicrous Amount of Data)\n",
    "\n",
    "<img src=\"https://i1.sndcdn.com/artworks-000223795050-2qhpbr-t500x500.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center;\">(Don't worry we won't use real Big Data)<p>\n",
    "    \n",
    "![sick](images/sick_comp.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data Needs to be Processed by a Team\n",
    "\n",
    "Ok, so my computer can't run it...\n",
    "\n",
    "Then, how is Big Data processed?\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce is an Approach to Process Big Data\n",
    "   \n",
    "Spark is based on a Divide and Conquer approach called MapReduce. MapReduce involves:\n",
    "* Keeping intermediate states\n",
    "* Identifying dead nodes\n",
    "* Labeling results to group them\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/199Q1.png\" alt=\"drawing\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example time!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The first thing we need to work with Spark is to define the Spark Context\n",
    "\n",
    "The SparkContext represents Spark's connetion with the cluster\n",
    "    \n",
    "https://sparkbyexamples.com/spark/spark-sparkcontext/\n",
    "\n",
    "![sc](https://raw.githubusercontent.com/LiberPH/Spark_para_que/main/spark_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize Spark Contex\n",
    "\n",
    "(we will use Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9563a4b7c342:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=pyspark-shell>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[4]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's read some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# https://www.kaggle.com/johnsmith88/heart-disease-dataset\n",
    "# Puede que tome un tiempo en una computadora local\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv('data/heart.csv', inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why it took so long?\n",
    "\n",
    "![meme](images/meme.jpeg)\n",
    "\n",
    "Remember: Spark divides the data for processing, that takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can make almost any SQL-like process (join, count, select, etc...) using Spark as well as some ML algorithms\n",
    "\n",
    "https://github.com/LiberPH/taller_spark\n",
    "\n",
    "![Spark](https://docs.snowflake.com/en/_images/spark-snowflake-data-source.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok, I just want to know what kind of data I have here without displaying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heart\n",
    "1. age\n",
    "2. sex\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholestoral in mg/dl\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n",
    "14. target: Is the patient cardiopath or not?\n",
    "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's get a look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|\n",
      "| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|\n",
      "| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|\n",
      "| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|\n",
      "| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|\n",
      "| 58|  0|  0|     100| 248|  0|      0|    122|    0|    1.0|    1|  0|   2|     1|\n",
      "| 58|  1|  0|     114| 318|  0|      2|    140|    0|    4.4|    0|  3|   1|     0|\n",
      "| 55|  1|  0|     160| 289|  0|      0|    145|    1|    0.8|    1|  1|   3|     0|\n",
      "| 46|  1|  0|     120| 249|  0|      0|    144|    0|    0.8|    2|  0|   3|     0|\n",
      "| 54|  1|  0|     122| 286|  0|      0|    116|    1|    3.2|    1|  2|   2|     0|\n",
      "| 71|  0|  0|     112| 149|  0|      1|    125|    0|    1.6|    1|  0|   2|     1|\n",
      "| 43|  0|  0|     132| 341|  1|      0|    136|    1|    3.0|    1|  0|   3|     0|\n",
      "| 34|  0|  1|     118| 210|  0|      1|    192|    0|    0.7|    2|  0|   2|     1|\n",
      "| 51|  1|  0|     140| 298|  0|      1|    122|    1|    4.2|    1|  3|   3|     0|\n",
      "| 52|  1|  0|     128| 204|  1|      1|    156|    1|    1.0|    1|  0|   0|     0|\n",
      "| 34|  0|  1|     118| 210|  0|      1|    192|    0|    0.7|    2|  0|   2|     1|\n",
      "| 51|  0|  2|     140| 308|  0|      0|    142|    0|    1.5|    2|  1|   2|     1|\n",
      "| 54|  1|  0|     124| 266|  0|      0|    109|    1|    2.2|    1|  1|   3|     0|\n",
      "| 50|  0|  1|     120| 244|  0|      1|    162|    0|    1.1|    2|  0|   2|     1|\n",
      "| 58|  1|  2|     140| 211|  1|      0|    165|    0|    0.0|    2|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can apply simple analytics with Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|         mean_age|mean_chol|\n",
      "+-----------------+---------+\n",
      "|54.43414634146342|    246.0|\n",
      "+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col as c\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df.select(\n",
    " F.avg(c(\"age\")).alias(\"mean_age\"), F.avg(c(\"chol\")).alias(\"mean_chol\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>54.43414634146342</td>\n",
       "      <td>0.6956097560975609</td>\n",
       "      <td>0.9424390243902439</td>\n",
       "      <td>131.61170731707318</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.14926829268292682</td>\n",
       "      <td>0.5297560975609756</td>\n",
       "      <td>149.11414634146342</td>\n",
       "      <td>0.33658536585365856</td>\n",
       "      <td>1.0715121951219524</td>\n",
       "      <td>1.3853658536585365</td>\n",
       "      <td>0.7541463414634146</td>\n",
       "      <td>2.32390243902439</td>\n",
       "      <td>0.5131707317073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>9.072290233244278</td>\n",
       "      <td>0.4603733241196495</td>\n",
       "      <td>1.029640743645865</td>\n",
       "      <td>17.516718005376408</td>\n",
       "      <td>51.59251020618203</td>\n",
       "      <td>0.35652668972715756</td>\n",
       "      <td>0.5278775668748918</td>\n",
       "      <td>23.00572374597721</td>\n",
       "      <td>0.4727723760037115</td>\n",
       "      <td>1.1750532551501767</td>\n",
       "      <td>0.6177552671745918</td>\n",
       "      <td>1.0307976650242825</td>\n",
       "      <td>0.6206602380510303</td>\n",
       "      <td>0.5000704980788011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>564</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                age                 sex                  cp  \\\n",
       "0   count               1025                1025                1025   \n",
       "1    mean  54.43414634146342  0.6956097560975609  0.9424390243902439   \n",
       "2  stddev  9.072290233244278  0.4603733241196495   1.029640743645865   \n",
       "3     min                 29                   0                   0   \n",
       "4     max                 77                   1                   3   \n",
       "\n",
       "             trestbps               chol                  fbs  \\\n",
       "0                1025               1025                 1025   \n",
       "1  131.61170731707318              246.0  0.14926829268292682   \n",
       "2  17.516718005376408  51.59251020618203  0.35652668972715756   \n",
       "3                  94                126                    0   \n",
       "4                 200                564                    1   \n",
       "\n",
       "              restecg             thalach                exang  \\\n",
       "0                1025                1025                 1025   \n",
       "1  0.5297560975609756  149.11414634146342  0.33658536585365856   \n",
       "2  0.5278775668748918   23.00572374597721   0.4727723760037115   \n",
       "3                   0                  71                    0   \n",
       "4                   2                 202                    1   \n",
       "\n",
       "              oldpeak               slope                  ca  \\\n",
       "0                1025                1025                1025   \n",
       "1  1.0715121951219524  1.3853658536585365  0.7541463414634146   \n",
       "2  1.1750532551501767  0.6177552671745918  1.0307976650242825   \n",
       "3                 0.0                   0                   0   \n",
       "4                 6.2                   2                   4   \n",
       "\n",
       "                 thal              target  \n",
       "0                1025                1025  \n",
       "1    2.32390243902439  0.5131707317073171  \n",
       "2  0.6206602380510303  0.5000704980788011  \n",
       "3                   0                   0  \n",
       "4                   3                   1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can clean your data using Spark\n",
    "\n",
    "The way to clean your data depends on many things:\n",
    "* How dirty is the data source?\n",
    "* How variables is your data?\n",
    "* What kind of data the ML algorithm you are planning to use supports?\n",
    "    * Null values\n",
    "    * Deviation https://deepnote.com/@rajshekar-2021/Outlier-Detection-Pyspark-069e69af-2c1d-4d4d-884a-92aad276d06f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ok, let's imagine we already cleaned our data\n",
    "![meme](images/wishful.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we want to use this data to find out if a person has a heart disease\n",
    "\n",
    "\n",
    "What kind of ML algorithm should we use?\n",
    "![ML learning](https://miro.medium.com/max/1200/1*FUZS9K4JPqzfXDcC83BQTw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The next step would be to select the features to use\n",
    "\n",
    "## Ok, let's imagine we already selected them XD\n",
    "![meme](images/wishful.jpeg)\n",
    "\n",
    "https://github.com/LiberPH/How_to_feature_selector_in_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kinds of algorithms](https://cdn-images-1.medium.com/max/800/1*rbaxTrB_CZCqbty_zv2bEg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's run the random forest algorithm to solve a classification problem\n",
    "\n",
    "![RandomForest](https://serokell.io/files/vz/vz1f8191.Ensemble-of-decision-trees.png)\n",
    "\n",
    "https://serokell.io/blog/random-forest-classification\n",
    "\n",
    "> Note: For this case we will skip some essential steps like:data cleaning, exploratory analysis, arelevant variables nalysis, among others. The aim is to show the general process to apply the Random Forest algorithm is spark to a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So, before to start we need to assemble our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[53.0, 1.0, 0.0, 140.0, 203.0, 1.0, 0.0, 155.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[70.0, 1.0, 0.0, 145.0, 174.0, 0.0, 1.0, 125.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  target\n",
       "0  [52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0...       0\n",
       "1  [53.0, 1.0, 0.0, 140.0, 203.0, 1.0, 0.0, 155.0...       0\n",
       "2  [70.0, 1.0, 0.0, 145.0, 174.0, 0.0, 1.0, 125.0...       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = df.columns\n",
    "features.remove(\"target\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols = features,outputCol='features')\n",
    "\n",
    "assembled = assembler.transform(df)\n",
    "\n",
    "#Just random variables and target\n",
    "data_rv = assembled.select('features','target')\n",
    "\n",
    "data_rv.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps to Run Random Forest\n",
    "\n",
    "1. Convert any categoric data to numeric\n",
    "2. Divide data set in two data sets: training and testing \n",
    "3. Add any missing `import`\n",
    "4. Train the model\n",
    "5. Test the model\n",
    "6. Check your metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Convert any categoric data to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Divide data set in two data sets: training and testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = data_rv.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Add any missing `import`\n",
    "https://github.com/LiberPH/taller_spark/blob/master/random_forest/RandomForest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Train model.\n",
    "model = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|prediction|         probability|            features|\n",
      "+----------+--------------------+--------------------+\n",
      "|       0.0|[0.58296473943002...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.13834998356016...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.13834998356016...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.05696006115714...|(13,[0,2,3,4,7,10...|\n",
      "|       1.0|[0.12727358073759...|(13,[0,2,3,4,7,10...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"probability\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Take a look at some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0673077\n"
     ]
    }
   ],
   "source": [
    "# Compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>target</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(41.0, 1.0, 0.0, 110.0, 172.0, 0.0, 0.0, 158.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.82964739430021, 4.17035260569979]</td>\n",
       "      <td>[0.582964739430021, 0.417035260569979]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.383499835601649, 8.616500164398351]</td>\n",
       "      <td>[0.1383499835601649, 0.8616500164398351]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.383499835601649, 8.616500164398351]</td>\n",
       "      <td>[0.1383499835601649, 0.8616500164398351]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53.0, 0.0, 2.0, 128.0, 216.0, 0.0, 0.0, 115.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5696006115714188, 9.430399388428581]</td>\n",
       "      <td>[0.05696006115714188, 0.9430399388428581]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(63.0, 0.0, 2.0, 135.0, 252.0, 0.0, 0.0, 172.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2727358073759316, 8.727264192624068]</td>\n",
       "      <td>[0.12727358073759315, 0.8727264192624068]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(49.0, 0.0, 0.0, 130.0, 269.0, 0.0, 1.0, 163.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8321779834464635, 9.167822016553536]</td>\n",
       "      <td>[0.08321779834464635, 0.9167822016553536]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(62.0, 0.0, 0.0, 124.0, 209.0, 0.0, 1.0, 163.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.8628797282981366, 8.137120271701864]</td>\n",
       "      <td>[0.18628797282981366, 0.8137120271701864]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(62.0, 0.0, 0.0, 124.0, 209.0, 0.0, 1.0, 163.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.8628797282981366, 8.137120271701864]</td>\n",
       "      <td>[0.18628797282981366, 0.8137120271701864]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3.04495228693082, 6.95504771306918]</td>\n",
       "      <td>[0.304495228693082, 0.695504771306918]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(62.0, 0.0, 0.0, 140.0, 268.0, 0.0, 0.0, 160.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.989272727240232, 2.010727272759768]</td>\n",
       "      <td>[0.7989272727240232, 0.20107272727597678]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(50.0, 0.0, 0.0, 110.0, 254.0, 0.0, 0.0, 159.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.1320123265881088, 8.867987673411891]</td>\n",
       "      <td>[0.11320123265881088, 0.8867987673411891]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(53.0, 0.0, 0.0, 138.0, 234.0, 0.0, 0.0, 160.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9993712607573879, 9.000628739242613]</td>\n",
       "      <td>[0.09993712607573879, 0.9000628739242613]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[29.0, 1.0, 1.0, 130.0, 204.0, 0.0, 0.0, 202.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.43544838813488174, 9.564551611865118]</td>\n",
       "      <td>[0.043544838813488175, 0.9564551611865119]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[34.0, 0.0, 1.0, 118.0, 210.0, 0.0, 1.0, 192.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5085531951742228, 9.49144680482578]</td>\n",
       "      <td>[0.050855319517422275, 0.9491446804825777]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[34.0, 1.0, 3.0, 118.0, 182.0, 0.0, 0.0, 174.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5122235321778932, 9.487776467822108]</td>\n",
       "      <td>[0.051222353217789306, 0.9487776467822107]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[34.0, 1.0, 3.0, 118.0, 182.0, 0.0, 0.0, 174.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5122235321778932, 9.487776467822108]</td>\n",
       "      <td>[0.051222353217789306, 0.9487776467822107]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[35.0, 0.0, 0.0, 138.0, 183.0, 0.0, 1.0, 182.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.698011316779797, 9.301988683220204]</td>\n",
       "      <td>[0.0698011316779797, 0.9301988683220204]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[35.0, 0.0, 0.0, 138.0, 183.0, 0.0, 1.0, 182.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.698011316779797, 9.301988683220204]</td>\n",
       "      <td>[0.0698011316779797, 0.9301988683220204]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[35.0, 1.0, 0.0, 120.0, 198.0, 0.0, 1.0, 130.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8.622693655307234, 1.3773063446927658]</td>\n",
       "      <td>[0.8622693655307234, 0.13773063446927658]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[35.0, 1.0, 0.0, 120.0, 198.0, 0.0, 1.0, 130.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8.622693655307234, 1.3773063446927658]</td>\n",
       "      <td>[0.8622693655307234, 0.13773063446927658]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  target  \\\n",
       "0   (41.0, 1.0, 0.0, 110.0, 172.0, 0.0, 0.0, 158.0...       0   \n",
       "1   (48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...       1   \n",
       "2   (48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...       1   \n",
       "3   (53.0, 0.0, 2.0, 128.0, 216.0, 0.0, 0.0, 115.0...       1   \n",
       "4   (63.0, 0.0, 2.0, 135.0, 252.0, 0.0, 0.0, 172.0...       1   \n",
       "5   (49.0, 0.0, 0.0, 130.0, 269.0, 0.0, 1.0, 163.0...       1   \n",
       "6   (62.0, 0.0, 0.0, 124.0, 209.0, 0.0, 1.0, 163.0...       1   \n",
       "7   (62.0, 0.0, 0.0, 124.0, 209.0, 0.0, 1.0, 163.0...       1   \n",
       "8   (46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...       1   \n",
       "9   (62.0, 0.0, 0.0, 140.0, 268.0, 0.0, 0.0, 160.0...       0   \n",
       "10  (50.0, 0.0, 0.0, 110.0, 254.0, 0.0, 0.0, 159.0...       1   \n",
       "11  (53.0, 0.0, 0.0, 138.0, 234.0, 0.0, 0.0, 160.0...       1   \n",
       "12  [29.0, 1.0, 1.0, 130.0, 204.0, 0.0, 0.0, 202.0...       1   \n",
       "13  [34.0, 0.0, 1.0, 118.0, 210.0, 0.0, 1.0, 192.0...       1   \n",
       "14  [34.0, 1.0, 3.0, 118.0, 182.0, 0.0, 0.0, 174.0...       1   \n",
       "15  [34.0, 1.0, 3.0, 118.0, 182.0, 0.0, 0.0, 174.0...       1   \n",
       "16  [35.0, 0.0, 0.0, 138.0, 183.0, 0.0, 1.0, 182.0...       1   \n",
       "17  [35.0, 0.0, 0.0, 138.0, 183.0, 0.0, 1.0, 182.0...       1   \n",
       "18  [35.0, 1.0, 0.0, 120.0, 198.0, 0.0, 1.0, 130.0...       0   \n",
       "19  [35.0, 1.0, 0.0, 120.0, 198.0, 0.0, 1.0, 130.0...       0   \n",
       "\n",
       "                               rawPrediction  \\\n",
       "0       [5.82964739430021, 4.17035260569979]   \n",
       "1     [1.383499835601649, 8.616500164398351]   \n",
       "2     [1.383499835601649, 8.616500164398351]   \n",
       "3    [0.5696006115714188, 9.430399388428581]   \n",
       "4    [1.2727358073759316, 8.727264192624068]   \n",
       "5    [0.8321779834464635, 9.167822016553536]   \n",
       "6    [1.8628797282981366, 8.137120271701864]   \n",
       "7    [1.8628797282981366, 8.137120271701864]   \n",
       "8       [3.04495228693082, 6.95504771306918]   \n",
       "9     [7.989272727240232, 2.010727272759768]   \n",
       "10   [1.1320123265881088, 8.867987673411891]   \n",
       "11   [0.9993712607573879, 9.000628739242613]   \n",
       "12  [0.43544838813488174, 9.564551611865118]   \n",
       "13    [0.5085531951742228, 9.49144680482578]   \n",
       "14   [0.5122235321778932, 9.487776467822108]   \n",
       "15   [0.5122235321778932, 9.487776467822108]   \n",
       "16    [0.698011316779797, 9.301988683220204]   \n",
       "17    [0.698011316779797, 9.301988683220204]   \n",
       "18   [8.622693655307234, 1.3773063446927658]   \n",
       "19   [8.622693655307234, 1.3773063446927658]   \n",
       "\n",
       "                                   probability  prediction  \n",
       "0       [0.582964739430021, 0.417035260569979]         0.0  \n",
       "1     [0.1383499835601649, 0.8616500164398351]         1.0  \n",
       "2     [0.1383499835601649, 0.8616500164398351]         1.0  \n",
       "3    [0.05696006115714188, 0.9430399388428581]         1.0  \n",
       "4    [0.12727358073759315, 0.8727264192624068]         1.0  \n",
       "5    [0.08321779834464635, 0.9167822016553536]         1.0  \n",
       "6    [0.18628797282981366, 0.8137120271701864]         1.0  \n",
       "7    [0.18628797282981366, 0.8137120271701864]         1.0  \n",
       "8       [0.304495228693082, 0.695504771306918]         1.0  \n",
       "9    [0.7989272727240232, 0.20107272727597678]         0.0  \n",
       "10   [0.11320123265881088, 0.8867987673411891]         1.0  \n",
       "11   [0.09993712607573879, 0.9000628739242613]         1.0  \n",
       "12  [0.043544838813488175, 0.9564551611865119]         1.0  \n",
       "13  [0.050855319517422275, 0.9491446804825777]         1.0  \n",
       "14  [0.051222353217789306, 0.9487776467822107]         1.0  \n",
       "15  [0.051222353217789306, 0.9487776467822107]         1.0  \n",
       "16    [0.0698011316779797, 0.9301988683220204]         1.0  \n",
       "17    [0.0698011316779797, 0.9301988683220204]         1.0  \n",
       "18   [0.8622693655307234, 0.13773063446927658]         0.0  \n",
       "19   [0.8622693655307234, 0.13773063446927658]         0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.limit(20).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The next step would be to feed the model with real data to test if patients suffer heart disease or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally, you must stop your spark session before closing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
